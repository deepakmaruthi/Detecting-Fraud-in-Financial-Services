# -*- coding: utf-8 -*-
"""Detecting Fraud in Financial Services.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KvP7D2uH6OzWEVek1IomQUrW3hpzkvW6
"""

# =============================================================================
# Project Setup: Initializing Environment and Exploring Dataset Directory
# Author: Anumula Deepak Maruthi
# Description: Custom environment setup for a financial fraud detection project
# =============================================================================

# Essential libraries for data handling and numerical operations
import numpy as np       # For numerical computations
import pandas as pd      # For data processing and manipulation

# Operating system utilities for file and directory management
import os

# Displaying all files present within the dataset directory
# (Directory path may vary depending on platform/environment)
dataset_path = '/kaggle/input'

print("Scanning dataset directory...\n")
for root_dir, _, file_list in os.walk(dataset_path):
    for file_name in file_list:
        print(f"Found: {os.path.join(root_dir, file_name)}")

# -------------------------------------------------------------------
# Notes:
# - You can use '/kaggle/working/' to store output files up to 20GB.
# - Temporary files can be stored in '/kaggle/temp/' but will be deleted once session ends.
# -------------------------------------------------------------------

# =============================================================================
# Module Imports for Modeling, Evaluation, and Visualization
# Author: Anumula Deepak Maruthi
# =============================================================================

# Scikit-learn: For model evaluation, cross-validation, and parameter tuning
from sklearn.model_selection import (
    train_test_split,
    StratifiedKFold,
    RepeatedStratifiedKFold,
    cross_val_score,
    GridSearchCV
)
from sklearn.metrics import classification_report, confusion_matrix

# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Deep Learning library
import tensorflow as tf

# Utilities for randomness and progress monitoring
import random
from statistics import mean
from tqdm.auto import tqdm  # Using auto for better compatibility in notebooks and scripts

# Suppress TensorFlow warnings for cleaner output (optional)
tf.get_logger().setLevel('ERROR')

# =============================================================================
# Handling Class Imbalance with SMOTE (Synthetic Minority Oversampling Technique)
# =============================================================================

# Imbalanced-learn library for advanced sampling techniques
import imblearn
from imblearn.over_sampling import SMOTE

# Utility for summarizing class distributions
from collections import Counter

# Display installed imbalanced-learn version (useful for reproducibility)
print(f"[INFO] imbalanced-learn version: {imblearn.__version__}")

# =============================================================================
# Importing Classification Models
# =============================================================================

# K-Nearest Neighbors: A distance-based classification algorithm
from sklearn.neighbors import KNeighborsClassifier

# XGBoost: An efficient and scalable gradient boosting framework
from xgboost import XGBClassifier

# =============================================================================
# Load Transaction Data from CSV File
# =============================================================================

# Define the path to the dataset (update if using a different environment)
data_file_path = '/content/PS_20174392719_1491204439457_log.csv'

# Read the dataset into a pandas DataFrame
df = pd.read_csv(data_file_path)

# Confirm successful load by showing basic info (optional)
print(f"[INFO] Data loaded successfully with {df.shape[0]} records and {df.shape[1]} features.")

# =============================================================================
# Filter Dataset for DEBIT Transactions Only
# =============================================================================

# Create a separate DataFrame containing only DEBIT transaction records
debit_transactions = df.query("type == 'DEBIT'").copy()

# Preview the filtered data (optional)
print(f"[INFO] Extracted {debit_transactions.shape[0]} DEBIT transactions.")
debit_transactions.head()

# =============================================================================
# Analyze Distribution of Fraudulent vs. Non-Fraudulent Transactions
# =============================================================================

# Count occurrences of each class in the 'isFraud' column
fraud_counts = df['isFraud'].value_counts().sort_index()

# Display the class distribution with context
print("[INFO] Transaction classification breakdown:")
for label, count in fraud_counts.items():
    status = "Fraudulent" if label == 1 else "Non-Fraudulent"
    print(f" - {status} transactions: {count}")

# =============================================================================
# Analyze Naming Pattern in Originator Accounts
# =============================================================================

# Check how many originator names do NOT begin with the letter 'M'
non_m_prefix_count = df['nameOrig'].apply(lambda x: not str(x).startswith('M')).sum()

# Display the result with explanation
print(f"[INFO] Number of originator accounts not starting with 'M': {non_m_prefix_count}")

# =============================================================================
# Count and Display Fraud vs. Non-Fraud Transaction Totals
# =============================================================================

# Separate counts for fraudulent and non-fraudulent transactions
fraud_total = (df['isFraud'] == 1).sum()
non_fraud_total = (df['isFraud'] == 0).sum()

# Print summary with context
print(f"[INFO] Total fraudulent transactions      : {fraud_total}")
print(f"[INFO] Total non-fraudulent transactions  : {non_fraud_total}")

# =============================================================================
# Dataset Structure Overview
# =============================================================================

# Display summary of dataset structure including column types and non-null counts
print("[INFO] Dataset Overview:")
print("-" * 50)
df_summary = df.info()

"""# Preprocessing

"""

# =============================================================================
# Count Unique Values Across All Columns
# =============================================================================

# Create a dictionary to store the number of unique entries per feature
unique_counts = {col: df[col].nunique() for col in df.columns}

# Display the unique value count for each column
print("[INFO] Unique value count by column:")
for col, count in unique_counts.items():
    print(f" - {col}: {count}")

# =============================================================================
# Identify Distinct Transaction Types
# =============================================================================

# Extract and display all unique transaction types present in the dataset
transaction_types = df['type'].dropna().unique()

print("[INFO] Available transaction types in dataset:")
for tx_type in transaction_types:
    print(f" - {tx_type}")

pd.get_dummies(df['type'],prefix='tp')

def onehot(df, column, prefix):
    df = df.copy()
    dummies = pd.get_dummies(df[column],prefix=prefix)
    df = pd.concat([df,dummies], axis=1)
    df = df.drop(column, axis=1)
    return df

def preprocessing(df):
    df = df.copy()

    df = df.drop(['step', 'isFlaggedFraud'],axis=1)

    #one-hot encode on type column
    df = onehot(df, column='type', prefix='tp')

    y = df['isFraud'].copy()
    X = df.drop('isFraud', axis=1).copy()

    # Train-Test Split
    #random state shows that the split shuffles the data always in the same way so you'll get the same data after each run
    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)

    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = preprocessing(df)

print(len(X_train), len(X_test))

X_train

counter = Counter(y_train)
print(counter)

categ_x_train = X_train[['nameOrig','nameDest']].copy()
X_train = X_train.drop(['nameOrig','nameDest'], axis=1)

categ_x_test = X_test[['nameOrig','nameDest']].copy()
X_test = X_test.drop(['nameOrig','nameDest'], axis=1)

X_test

oversample = SMOTE()
X_train, y_train = oversample.fit_resample(X_train, y_train)
X_train = X_train.sample(frac=1.0,random_state=123).reset_index(drop=True)
y_train = y_train.sample(frac=1.0,random_state=123).reset_index(drop=True)
counter = Counter(y_train)
print(counter)

# KNN
knn = KNeighborsClassifier(n_neighbors=10)
model=knn.fit(X_train, y_train)
pred = model.predict(X_test)
pred

# XG Boost
model = XGBClassifier(n_jobs=-1)


# # summarize performance
# print('Mean ROC AUC: %.5f' % mean(scores))

# define evaluation procedure
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
cv

# # evaluate model
for i in tqdm(range(1)):
    scores = cross_val_score(model, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)

# summarize performance
print('Mean F1: %.5f' % mean(scores))

model.fit(X_train, y_train)

import joblib


filename = 'finalized_model.sav'

joblib.dump(model, filename)


# load the model from disk

loaded_model = joblib.load(filename)

result = loaded_model.predict(X_test)

print(result)

y_pred = model.predict(X_test)

cm = confusion_matrix(y_test,y_pred)
clr = classification_report(y_test, y_pred, target_names=['Not Fraud','Fraud'])
cm

plt.figure(figsize=(8, 8))
sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')
plt.xticks(np.arange(2)+0.5, ['Not Fraud','Fraud'])
plt.yticks(np.arange(2)+0.5, ['Not Fraud','Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

print('Classification Report:\n', clr)

! git clone https://github.com/gradio-app/gradio.git

! ls gradio/

! python gradio/setup.py install

! pip install gradio

! python gradio/setup.py install

import gradio as gr

def dataframe(file_obj):
    df = pd.read_csv(file_obj.name)
    df = onehot(df, column='type', prefix='tp')
    df = df.drop(['nameOrig','nameDest','step'], axis=1)
    print(df.shape)
    y_pred = model.predict(df)

    pred_df = pd.DataFrame(y_pred, columns = ['isFraud'])
    print(type(pred_df))
    print(pred_df.shape)
#     clr = classification_report(y_test, y_pred, target_names=['Not Fraud','Fraud'])
#     return 'Classification Report:\n'+ clr
    return pred_df

# =============================================================================
# Gradio Interface for Batch Fraud Prediction from CSV File
# =============================================================================

import gradio as gr

# Define file input: accept only a single CSV file
csv_input = gr.File(label="Upload Transaction CSV", file_types=[".csv"])

# Define DataFrame output: display prediction results
prediction_output = gr.Dataframe(label="Predicted Outcomes")

# Create Gradio Interface
fraud_detection_ui = gr.Interface(
    fn=dataframe,  # Make sure to define this function separately
    inputs=csv_input,
    outputs=prediction_output,
    title="📊 Mobile Money Fraud Detection System",
    description="Upload a CSV file with transaction records to detect potential fraud.",
    theme="soft"  # Use 'soft', 'default', or custom themes; 'dark-peach' may be unsupported
)

# Launch the interface
fraud_detection_ui.launch()

fraud_detection_ui.launch(share=True)

y = np.array([0,1])
y[0]

# =============================================================================
# Single Transaction Fraud Prediction Function
# =============================================================================

def predict_transaction(trans_type, amount, old_balance_origin):
    """
    Predicts the likelihood of a single transaction being fraudulent
    based on transaction type, amount, and originator's old balance.
    """

    # Dynamically compute new originator balance based on transaction type
    balance_update_logic = {
        "PAYMENT": old_balance_origin - amount,
        "TRANSFER": old_balance_origin - amount,
        "CASH_OUT": old_balance_origin - amount,
        "CASH_IN":  old_balance_origin + amount,
        "DEBIT":    old_balance_origin - amount,
    }
    new_balance_origin = balance_update_logic.get(trans_type, 0.0)

    # Construct feature dictionary for the model
    sample = {
        'type': trans_type,
        'amount': amount,
        'oldbalanceOrg': old_balance_origin,
        'newbalanceOrig': new_balance_origin,
        'oldbalanceDest': 0.0,
        'newbalanceDest': 0.0,
        'tp_PAYMENT': 0,
        'tp_TRANSFER': 0,
        'tp_CASH_OUT': 0,
        'tp_CASH_IN': 0,
        'tp_DEBIT': 0
    }

    # Convert to DataFrame
    input_df = pd.DataFrame(sample, index=[0])

    # Remove the one-hot flag for the selected type (to be re-added properly)
    type_column_to_drop = f"tp_{trans_type}"
    if type_column_to_drop in input_df.columns:
        input_df = input_df.drop(columns=type_column_to_drop)

    # Apply one-hot encoding for 'type'
    input_df = onehot(input_df, column='type', prefix='tp')

    # Log processed input (for debugging)
    print("[DEBUG] Processed Input for Model:")
    print(input_df)

    # Run prediction using preloaded model
    probabilities = model.predict_proba(input_df)[0].tolist()

    # Create named probability output
    prediction = {
        "Not Fraud": probabilities[0],
        "Fraud": probabilities[1]
    }

    return prediction, new_balance_origin

# =============================================================================
# Gradio Interface for Real-Time Fraud Prediction (Single Transaction)
# =============================================================================

import gradio as gr

# Input components
transaction_type_input = gr.Dropdown(
    choices=['PAYMENT', 'TRANSFER', 'CASH_OUT', 'CASH_IN', 'DEBIT'],
    label="Transaction Type",
    value=None,
    interactive=True
)

transaction_amount_input = gr.Number(
    label="Transaction Amount",
    value=None
)

origin_balance_input = gr.Number(
    label="Originator's Old Balance",
    value=100000
)

# Output components
fraud_label_output = gr.Label(
    label="Fraud Prediction"
)

predicted_new_balance_output = gr.Textbox(
    label="Predicted New Balance (Originator)"
)

# Define interface
per_transaction_interface = gr.Interface(
    fn=predict_transaction,  # This should match your function name
    inputs=[transaction_type_input, transaction_amount_input, origin_balance_input],
    outputs=[fraud_label_output, predicted_new_balance_output],
    title="💸 Oddity: Intelligent Fraud Detection for Mobile Transactions",
    description=(
        "Mobile Money Transactions (MMTs) have revolutionized financial access, yet remain vulnerable to fraud. "
        "This tool predicts the likelihood of a transaction being fraudulent based on key parameters. "
        "Empowering users with AI-driven insights helps build a safer, more inclusive FinTech ecosystem."
    ),
    theme="soft"  # 'dark-peach' is not a supported Gradio v4+ theme. Use 'soft', 'default', or omit.
)

per_transaction_interface.launch(share=True)

"""Step-by-Step: Deploy Gradio App Permanently to Hugging Face **Spaces**"""

pip install gradio huggingface_hub

!huggingface-cli login

# Commented out IPython magic to ensure Python compatibility.
!mkdir fraud-detection-app
# %cd fraud-detection-app

import joblib

# Replace this with your actual trained model object
joblib.dump(model, "model.pkl")

import gradio as gr
import pandas as pd
import joblib

model = joblib.load("model.pkl")  # Ensure this file exists in the same directory

def onehot(df, column, prefix):
    one_hot = pd.get_dummies(df[column], prefix=prefix)
    df = df.drop(column, axis=1)
    return pd.concat([df, one_hot], axis=1)

def predict_transaction(trans_type, amount, old_balance_origin):
    balance_update = {
        "PAYMENT": old_balance_origin - amount,
        "TRANSFER": old_balance_origin - amount,
        "CASH_OUT": old_balance_origin - amount,
        "CASH_IN":  old_balance_origin + amount,
        "DEBIT":    old_balance_origin - amount,
    }
    new_balance_origin = balance_update.get(trans_type, 0.0)

    row = {
        'type': trans_type,
        'amount': amount,
        'oldbalanceOrg': old_balance_origin,
        'newbalanceOrig': new_balance_origin,
        'oldbalanceDest': 0.0,
        'newbalanceDest': 0.0,
        'tp_PAYMENT': 0,
        'tp_TRANSFER': 0,
        'tp_CASH_OUT': 0,
        'tp_CASH_IN': 0,
        'tp_DEBIT': 0
    }

    df = pd.DataFrame(row, index=[0])
    drop_col = f"tp_{trans_type}"
    if drop_col in df.columns:
        df = df.drop(columns=drop_col)
    df = onehot(df, 'type', 'tp')
    preds = model.predict_proba(df)[0].tolist()
    return {"Not Fraud": preds[0], "Fraud": preds[1]}, new_balance_origin

interface = gr.Interface(
    fn=predict_transaction,
    inputs=[
        gr.Dropdown(['PAYMENT','TRANSFER','CASH_OUT','CASH_IN','DEBIT'], label="Transaction Type"),
        gr.Number(label="Transaction Amount"),
        gr.Number(label="Originator's Old Balance", value=100000)
    ],
    outputs=[
        gr.Label(label="Fraud Probability"),
        gr.Textbox(label="Predicted New Balance")
    ],
    title="📊 Mobile Fraud Detection App",
    description="Upload transaction details and detect fraud in real time."
)

interface.launch()

"""Add Required Files



app.py — your main code (as above)
requirements.txt — dependencies:
gradio
pandas
scikit-learn
joblib
If you're using XGBoost or TensorFlow, add them as well.

"""

!gradio deploy

from huggingface_hub import notebook_login
notebook_login()